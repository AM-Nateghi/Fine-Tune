"""
Data Flattening Script - JSONL Input Version
=============================================
ØªØ¨Ø¯ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø± nested Ø¨Ù‡ flat dataset Ø¨Ø±Ø§ÛŒ SFT Ùˆ DPO

Input Structure (JSONL - Ù‡Ø± Ø®Ø· ÛŒÚ© JSON):
{
  "question_id": "...",
  "positive_responses": [{"text": "...", "score_ratio": 1.0}, ...],
  "negative_responses": [{"text": "...", "score_ratio": 0.0}, ...],
  "questions": ["Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ", "variant 1", ...]
}

Output:
- sft_dataset.jsonl: Ø¨Ø±Ø§ÛŒ Supervised Fine-Tuning
- dpo_dataset.jsonl: Ø¨Ø±Ø§ÛŒ Direct Preference Optimization
- dataset_stats.json: Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„
"""

import json
import random
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict, Counter
import numpy as np


# ================================================================
# Configuration
# ================================================================
class Config:
    INPUT_FILE = "assets/merged_dataset.jsonl"  # ğŸ”¥ Ø­Ø§Ù„Ø§ JSONL
    OUTPUT_DIR = "assets/flattened"

    # SFT Settings
    SFT_OUTPUT = "sft_dataset.jsonl"
    SFT_MIN_SCORE = 0.8  # Ø­Ø¯Ø§Ù‚Ù„ score Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª

    # DPO Settings
    DPO_OUTPUT = "dpo_dataset.jsonl"
    DPO_MIN_POSITIVE_SCORE = 0.9  # Ø­Ø¯Ø§Ù‚Ù„ score Ø¨Ø±Ø§ÛŒ chosen
    DPO_MAX_NEGATIVE_SCORE = 0.3  # Ø­Ø¯Ø§Ú©Ø«Ø± score Ø¨Ø±Ø§ÛŒ rejected
    DPO_PAIRING_STRATEGY = "random"  # "random", "best_worst", "score_based"

    # Balancing
    ENABLE_BALANCING = True
    MAX_SAMPLES_PER_QUESTION = 1000  # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØºÙ„Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù¾Ø±Ù¾Ø§Ø³Ø® ÙˆÙ„ÛŒ Ø®Ø¨ ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆØ§Ù„Ø§Øª Ø®ÛŒÙ„ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø² Ø§ÛŒÙ† Ø¹Ø¯Ø¯Ù‡
    MIN_SAMPLES_PER_QUESTION = 5  # Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ø±Ø§ÛŒ DPO pairing

    # Quality Control
    MIN_TEXT_LENGTH = 10  # Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„ Ù…ØªÙ† (Ú©Ø§Ø±Ø§Ú©ØªØ±)
    MAX_TEXT_LENGTH = 2000  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù…ØªÙ†

    SEED = 42


config = Config()
random.seed(config.SEED)
np.random.seed(config.SEED)


# ================================================================
# Text Normalization
# ================================================================
ARABIC_TO_PERSIAN = {"\u064a": "ÛŒ", "\u0643": "Ú©"}
PERSIAN_DIGITS_TO_EN = {f"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹"[i]: str(i) for i in range(10)}


def normalize_text(text: str) -> str:
    """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    if not isinstance(text, str):
        return ""

    # ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
    for ar, fa in ARABIC_TO_PERSIAN.items():
        text = text.replace(ar, fa)

    # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
    for fa, en in PERSIAN_DIGITS_TO_EN.items():
        text = text.replace(fa, en)

    # Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = " ".join(text.split())

    # Ø§ØµÙ„Ø§Ø­ Ù†Ø´Ø§Ù†Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ
    text = text.replace(" ØŸ", "ØŸ").replace("ØŒ", "ØŒ ")

    return text.strip()


def is_valid_text(text: str) -> bool:
    """Ú†Ú© Ú©Ø±Ø¯Ù† validity Ù…ØªÙ†"""
    if not text or not isinstance(text, str):
        return False

    text_len = len(text)
    if text_len < config.MIN_TEXT_LENGTH or text_len > config.MAX_TEXT_LENGTH:
        return False

    # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ù…ØªÙ† ÙÙ‚Ø· Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¹Ø¬ÛŒØ¨ Ù†Ø¨Ø§Ø´Ù‡
    if len(text.strip()) < 5:
        return False

    return True


# ================================================================
# Load JSONL Data
# ================================================================
def load_jsonl_data() -> List[Dict]:
    """
    Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ JSONL (Ù‡Ø± Ø®Ø· ÛŒÚ© JSON object)
    """
    print(f"\nğŸ“‚ Loading data from: {config.INPUT_FILE}")

    data = []
    with open(config.INPUT_FILE, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue

            try:
                item = json.loads(line)
                data.append(item)
            except json.JSONDecodeError as e:
                print(f"   âš ï¸  Warning: Invalid JSON at line {line_num}: {e}")
                continue

    print(f"   âœ… Loaded {len(data)} questions")
    return data


# ================================================================
# SFT Dataset Generation
# ================================================================
def generate_sft_dataset(data: List[Dict]) -> List[Dict]:
    """
    ØªÙˆÙ„ÛŒØ¯ dataset Ø¨Ø±Ø§ÛŒ SFT
    Format: {"question": "...", "response": "...", "score": 1.0, "weight": 0.5}
    """
    print("\n" + "=" * 70)
    print("ğŸ“š GENERATING SFT DATASET")
    print("=" * 70)

    sft_samples = []
    question_response_counts = Counter()
    skipped = {"invalid_text": 0, "low_score": 0, "no_questions": 0}

    for item_idx, item in enumerate(data):
        question_variants = item.get("questions", [])
        if not question_variants:
            skipped["no_questions"] += 1
            continue

        positive_responses = item.get("positive_responses", [])
        question_id = item.get("question_id", f"q_{item_idx}")

        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§
        if len(positive_responses) > config.MAX_SAMPLES_PER_QUESTION:
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§ Ø¨Ø±Ø§Ø³Ø§Ø³ score
            positive_responses = sorted(
                positive_responses, key=lambda x: x.get("score_ratio", 0), reverse=True
            )[: config.MAX_SAMPLES_PER_QUESTION]

        valid_samples_for_question = 0

        for resp in positive_responses:
            score = resp.get("score_ratio", 0)

            # ÙÛŒÙ„ØªØ± score
            if score < config.SFT_MIN_SCORE:
                skipped["low_score"] += 1
                continue

            response_text = normalize_text(resp.get("text", ""))

            # Validation
            if not is_valid_text(response_text):
                skipped["invalid_text"] += 1
                continue

            # Ø§Ù†ØªØ®Ø§Ø¨ ÛŒÚ© variant Ø±Ù†Ø¯ÙˆÙ… Ø§Ø² Ø³ÙˆØ§Ù„
            question_text = normalize_text(random.choice(question_variants))

            if not is_valid_text(question_text):
                skipped["invalid_text"] += 1
                continue

            sft_samples.append(
                {
                    "question": question_text,
                    "response": response_text,
                    "score": float(score),
                    "question_id": question_id,
                    "weight": 1.0,  # Ø¨Ø¹Ø¯Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´Ù‡
                }
            )

            valid_samples_for_question += 1

        question_response_counts[question_id] = valid_samples_for_question

    # ================================================================
    # Weight Balancing
    # ================================================================
    if config.ENABLE_BALANCING and sft_samples:
        print("\nâš–ï¸  Calculating sample weights for balancing...")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§
        response_counts = list(question_response_counts.values())
        avg_responses = np.mean(response_counts)

        print(f"   Average responses per question: {avg_responses:.1f}")
        print(f"   Min: {min(response_counts)}, Max: {max(response_counts)}")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ weight Ø¨Ø±Ø§ÛŒ Ù‡Ø± sample
        for sample in sft_samples:
            q_id = sample["question_id"]
            count = question_response_counts[q_id]

            # weight Ù…Ø¹Ú©ÙˆØ³ ØªØ¹Ø¯Ø§Ø¯ (Ø³ÙˆØ§Ù„Ø§Øª Ú©Ù…â€ŒÙ¾Ø§Ø³Ø® ÙˆØ²Ù† Ø¨ÛŒØ´ØªØ± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†)
            sample["weight"] = avg_responses / count if count > 0 else 1.0

        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ weights
        total_weight = sum(s["weight"] for s in sft_samples)
        for sample in sft_samples:
            sample["weight"] = sample["weight"] / total_weight * len(sft_samples)

        weight_stats = [s["weight"] for s in sft_samples]
        print(f"   Weight range: {min(weight_stats):.3f} - {max(weight_stats):.3f}")

    print(f"\nâœ… SFT Samples: {len(sft_samples):,}")
    print(f"   Skipped - Invalid text: {skipped['invalid_text']:,}")
    print(f"   Skipped - Low score: {skipped['low_score']:,}")
    print(f"   Skipped - No questions: {skipped['no_questions']:,}")

    return sft_samples


# ================================================================
# DPO Dataset Generation
# ================================================================
def generate_dpo_dataset(data: List[Dict]) -> List[Dict]:
    """
    ØªÙˆÙ„ÛŒØ¯ dataset Ø¨Ø±Ø§ÛŒ DPO
    Format: {"question": "...", "chosen": "...", "rejected": "...", "weight": 0.5}

    skipped Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ: Ø°Ø®ÛŒØ±Ù‡ ØªØ¹Ø¯Ø§Ø¯ samples Ú©Ù‡ skip Ø´Ø¯Ù† Ø¨Ù‡ Ø¯Ù„Ø§ÛŒÙ„ Ù…Ø®ØªÙ„Ù:
    - no_positives: Ø³ÙˆØ§Ù„Ø§ØªÛŒ Ú©Ù‡ Ù¾Ø§Ø³Ø® Ù…Ø«Ø¨Øª Ù…Ø¹ØªØ¨Ø± Ù†Ø¯Ø§Ø±Ù†
    - no_negatives: Ø³ÙˆØ§Ù„Ø§ØªÛŒ Ú©Ù‡ Ù¾Ø§Ø³Ø® Ù…Ù†ÙÛŒ Ù…Ø¹ØªØ¨Ø± Ù†Ø¯Ø§Ø±Ù†
    - invalid_text: Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ validation Ø±Ùˆ pass Ù†Ú©Ø±Ø¯Ù†
    - insufficient_samples: Ø³ÙˆØ§Ù„Ø§ØªÛŒ Ú©Ù‡ Ú©Ù…ØªØ± Ø§Ø² MIN_SAMPLES_PER_QUESTION pair Ø¯Ø§Ø±Ù†
    """
    print("\n" + "=" * 70)
    print("ğŸ”€ GENERATING DPO DATASET")
    print("=" * 70)

    dpo_samples = []
    question_pair_counts = Counter()

    # ğŸ”¥ Ø§ÛŒÙ† Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªØ¹Ø¯Ø§Ø¯ samples Ú©Ù‡ Ø¨Ù‡ Ø¯Ù„Ø§ÛŒÙ„ Ù…Ø®ØªÙ„Ù skip Ø´Ø¯Ù† Ø±Ùˆ Ù†Ú¯Ù‡ Ù…ÛŒØ¯Ø§Ø±Ù‡
    skipped = {
        "no_positives": 0,  # Ø³ÙˆØ§Ù„Ø§ØªÛŒ Ú©Ù‡ Ù¾Ø§Ø³Ø® Ù…Ø«Ø¨Øª Ù…Ø¹ØªØ¨Ø± Ù†Ø¯Ø§Ø±Ù†
        "no_negatives": 0,  # Ø³ÙˆØ§Ù„Ø§ØªÛŒ Ú©Ù‡ Ù¾Ø§Ø³Ø® Ù…Ù†ÙÛŒ Ù…Ø¹ØªØ¨Ø± Ù†Ø¯Ø§Ø±Ù†
        "invalid_text": 0,  # Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡/Ø¨Ù„Ù†Ø¯ ÛŒØ§ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ù†
        "insufficient_samples": 0,  # Ø³ÙˆØ§Ù„Ø§ØªÛŒ Ú©Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² pairing Ú©Ù…ØªØ± Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² pair Ø¯Ø§Ø´ØªÙ†
    }

    for item_idx, item in enumerate(data):
        question_variants = item.get("questions", [])
        if not question_variants:
            continue

        question_id = item.get("question_id", f"q_{item_idx}")

        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª (chosen)
        # Ø¨Ø§ÛŒØ¯ score Ø¨Ø§Ù„Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù† Ùˆ Ù…ØªÙ† Ù…Ø¹ØªØ¨Ø± Ø¨Ø§Ø´Ù‡
        positive_responses = [
            r
            for r in item.get("positive_responses", [])
            if r.get("score_ratio", 0) >= config.DPO_MIN_POSITIVE_SCORE
            and is_valid_text(normalize_text(r.get("text", "")))
        ]

        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…Ù†ÙÛŒ (rejected)
        # Ø¨Ø§ÛŒØ¯ score Ù¾Ø§ÛŒÛŒÙ† Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù† Ùˆ Ù…ØªÙ† Ù…Ø¹ØªØ¨Ø± Ø¨Ø§Ø´Ù‡
        negative_responses = [
            r
            for r in item.get("negative_responses", [])
            if r.get("score_ratio", 1) <= config.DPO_MAX_NEGATIVE_SCORE
            and is_valid_text(normalize_text(r.get("text", "")))
        ]

        # Ø§Ú¯Ù‡ Ù¾Ø§Ø³Ø® Ù…Ø«Ø¨Øª Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ùˆ skip Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if not positive_responses:
            skipped["no_positives"] += 1
            continue

        # Ø§Ú¯Ù‡ Ù¾Ø§Ø³Ø® Ù…Ù†ÙÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ùˆ skip Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if not negative_responses:
            skipped["no_negatives"] += 1
            continue

        # Pairing: Ø³Ø§Ø®Øª Ø¬ÙØªâ€ŒÙ‡Ø§ÛŒ (Ù…Ø«Ø¨ØªØŒ Ù…Ù†ÙÛŒ)
        pairs = create_pairs(
            positive_responses, negative_responses, config.DPO_PAIRING_STRATEGY
        )

        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ pairs (Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ù„Ø§Ù†Ø³)
        if len(pairs) > config.MAX_SAMPLES_PER_QUESTION:
            pairs = random.sample(pairs, config.MAX_SAMPLES_PER_QUESTION)

        # Ø§Ú¯Ù‡ Ø®ÛŒÙ„ÛŒ Ú©Ù… pair Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ùˆ skip Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if len(pairs) < config.MIN_SAMPLES_PER_QUESTION:
            skipped["insufficient_samples"] += 1
            continue

        # Ø³Ø§Ø®Øª samples Ù†Ù‡Ø§ÛŒÛŒ
        for pos_resp, neg_resp in pairs:
            question_text = normalize_text(random.choice(question_variants))
            chosen_text = normalize_text(pos_resp.get("text", ""))
            rejected_text = normalize_text(neg_resp.get("text", ""))

            dpo_samples.append(
                {
                    "question": question_text,
                    "chosen": chosen_text,
                    "rejected": rejected_text,
                    "chosen_score": float(pos_resp.get("score_ratio", 1.0)),
                    "rejected_score": float(neg_resp.get("score_ratio", 0.0)),
                    "question_id": question_id,
                    "weight": 1.0,
                }
            )

        question_pair_counts[question_id] = len(pairs)

    # ================================================================
    # Weight Balancing
    # ================================================================
    if config.ENABLE_BALANCING and dpo_samples:
        print("\nâš–ï¸  Calculating sample weights for balancing...")

        pair_counts = list(question_pair_counts.values())
        avg_pairs = np.mean(pair_counts)

        print(f"   Average pairs per question: {avg_pairs:.1f}")
        print(f"   Min: {min(pair_counts)}, Max: {max(pair_counts)}")

        for sample in dpo_samples:
            q_id = sample["question_id"]
            count = question_pair_counts[q_id]
            sample["weight"] = avg_pairs / count if count > 0 else 1.0

        total_weight = sum(s["weight"] for s in dpo_samples)
        for sample in dpo_samples:
            sample["weight"] = sample["weight"] / total_weight * len(dpo_samples)

        weight_stats = [s["weight"] for s in dpo_samples]
        print(f"   Weight range: {min(weight_stats):.3f} - {max(weight_stats):.3f}")

    print(f"\nâœ… DPO Pairs: {len(dpo_samples):,}")
    print(f"   Skipped - No positives: {skipped['no_positives']:,}")
    print(f"   Skipped - No negatives: {skipped['no_negatives']:,}")
    print(f"   Skipped - Insufficient samples: {skipped['insufficient_samples']:,}")

    return dpo_samples


def create_pairs(
    positive_responses: List[Dict], negative_responses: List[Dict], strategy: str
) -> List[tuple]:
    """Ø§ÛŒØ¬Ø§Ø¯ pairs Ø§Ø² chosen Ùˆ rejected"""

    if strategy == "random":
        # Ø±Ù†Ø¯ÙˆÙ… pairing
        n_pairs = min(len(positive_responses), len(negative_responses))
        pos_shuffled = random.sample(positive_responses, n_pairs)
        neg_shuffled = random.sample(negative_responses, n_pairs)
        return list(zip(pos_shuffled, neg_shuffled))

    elif strategy == "best_worst":
        # Ø¨Ù‡ØªØ±ÛŒÙ† positive Ø¨Ø§ Ø¨Ø¯ØªØ±ÛŒÙ† negative
        pos_sorted = sorted(
            positive_responses, key=lambda x: x.get("score_ratio", 0), reverse=True
        )
        neg_sorted = sorted(negative_responses, key=lambda x: x.get("score_ratio", 1))
        n_pairs = min(len(pos_sorted), len(neg_sorted))
        return list(zip(pos_sorted[:n_pairs], neg_sorted[:n_pairs]))

    elif strategy == "score_based":
        # Pairing Ø¨Ø±Ø§Ø³Ø§Ø³ Ø§Ø®ØªÙ„Ø§Ù score
        pairs = []
        for pos in positive_responses:
            for neg in negative_responses:
                score_diff = pos.get("score_ratio", 1) - neg.get("score_ratio", 0)
                if score_diff > 0.5:  # Ø­Ø¯Ø§Ù‚Ù„ 0.5 Ø§Ø®ØªÙ„Ø§Ù
                    pairs.append((pos, neg))

        # Ø§Ú¯Ø± Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯ Ø´Ø¯ØŒ sample Ú©Ù†
        if len(pairs) > config.MAX_SAMPLES_PER_QUESTION:
            pairs = random.sample(pairs, config.MAX_SAMPLES_PER_QUESTION)

        return pairs

    else:
        raise ValueError(f"Unknown pairing strategy: {strategy}")


# ================================================================
# Statistics
# ================================================================
def calculate_statistics(sft_samples: List[Dict], dpo_samples: List[Dict]) -> Dict:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„"""

    stats = {
        "sft": {
            "total_samples": len(sft_samples),
            "unique_questions": len(set(s["question_id"] for s in sft_samples)),
            "avg_response_length": (
                np.mean([len(s["response"]) for s in sft_samples]) if sft_samples else 0
            ),
            "score_distribution": {},
            "weight_distribution": {},
        },
        "dpo": {
            "total_pairs": len(dpo_samples),
            "unique_questions": len(set(s["question_id"] for s in dpo_samples)),
            "avg_chosen_length": (
                np.mean([len(s["chosen"]) for s in dpo_samples]) if dpo_samples else 0
            ),
            "avg_rejected_length": (
                np.mean([len(s["rejected"]) for s in dpo_samples]) if dpo_samples else 0
            ),
            "weight_distribution": {},
        },
    }

    # Score distribution for SFT
    if sft_samples:
        scores = [s["score"] for s in sft_samples]
        stats["sft"]["score_distribution"] = {
            "min": float(min(scores)),
            "max": float(max(scores)),
            "mean": float(np.mean(scores)),
            "median": float(np.median(scores)),
        }

        weights = [s["weight"] for s in sft_samples]
        stats["sft"]["weight_distribution"] = {
            "min": float(min(weights)),
            "max": float(max(weights)),
            "mean": float(np.mean(weights)),
        }

    # Weight distribution for DPO
    if dpo_samples:
        weights = [s["weight"] for s in dpo_samples]
        stats["dpo"]["weight_distribution"] = {
            "min": float(min(weights)),
            "max": float(max(weights)),
            "mean": float(np.mean(weights)),
        }

    return stats


# ================================================================
# Main Execution
# ================================================================
def main():
    print("\n" + "=" * 70)
    print("ğŸš€ DATA FLATTENING SCRIPT - JSONL VERSION")
    print("=" * 70)

    # Ø§ÛŒØ¬Ø§Ø¯ output directory
    output_dir = Path(config.OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ğŸ”¥ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯ÛŒØªØ§ÛŒ JSONL
    data = load_jsonl_data()

    # ØªÙˆÙ„ÛŒØ¯ SFT dataset
    sft_samples = generate_sft_dataset(data)

    # ØªÙˆÙ„ÛŒØ¯ DPO dataset
    dpo_samples = generate_dpo_dataset(data)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
    print("\n" + "=" * 70)
    print("ğŸ“Š CALCULATING STATISTICS")
    print("=" * 70)
    stats = calculate_statistics(sft_samples, dpo_samples)

    print(f"\nâœ… SFT Statistics:")
    print(f"   Total samples: {stats['sft']['total_samples']:,}")
    print(f"   Unique questions: {stats['sft']['unique_questions']}")
    print(f"   Avg response length: {stats['sft']['avg_response_length']:.0f} chars")
    print(
        f"   Score range: {stats['sft']['score_distribution'].get('min', 0):.2f} - {stats['sft']['score_distribution'].get('max', 0):.2f}"
    )

    print(f"\nâœ… DPO Statistics:")
    print(f"   Total pairs: {stats['dpo']['total_pairs']:,}")
    print(f"   Unique questions: {stats['dpo']['unique_questions']}")
    print(f"   Avg chosen length: {stats['dpo']['avg_chosen_length']:.0f} chars")
    print(f"   Avg rejected length: {stats['dpo']['avg_rejected_length']:.0f} chars")

    # Shuffle
    print("\nğŸ”€ Shuffling datasets...")
    random.shuffle(sft_samples)
    random.shuffle(dpo_samples)

    # Ø°Ø®ÛŒØ±Ù‡ SFT
    sft_path = output_dir / config.SFT_OUTPUT
    print(f"\nğŸ’¾ Saving SFT dataset to: {sft_path}")
    with open(sft_path, "w", encoding="utf-8") as f:
        for sample in sft_samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    print(f"   âœ… Saved {len(sft_samples):,} samples")

    # Ø°Ø®ÛŒØ±Ù‡ DPO
    dpo_path = output_dir / config.DPO_OUTPUT
    print(f"\nğŸ’¾ Saving DPO dataset to: {dpo_path}")
    with open(dpo_path, "w", encoding="utf-8") as f:
        for sample in dpo_samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    print(f"   âœ… Saved {len(dpo_samples):,} pairs")

    # Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù…Ø§Ø±
    stats_path = output_dir / "dataset_stats.json"
    print(f"\nğŸ’¾ Saving statistics to: {stats_path}")
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 70)
    print("âœ… FLATTENING COMPLETE!")
    print(f"   SFT: {sft_path}")
    print(f"   DPO: {dpo_path}")
    print(f"   Stats: {stats_path}")
    print("=" * 70)


if __name__ == "__main__":
    main()
