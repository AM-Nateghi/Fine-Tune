{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a35493f",
   "metadata": {},
   "source": [
    "use this enviroment to prevent crash the model when it's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e77e5608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from peft import LoraConfig, PeftConfig, get_peft_model, TaskType\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "dataset = datasets.load_dataset(\"knkarthick/dialogsum\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddacbf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 autocast پشتیبانی نمی‌شود: autocast.__init__() got an unexpected keyword argument 'device_type'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4570/3720787465.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# try a simple test to check autocast with bfloat16\n",
    "try:\n",
    "    with torch.cuda.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        x = torch.randn(1, device=\"cuda\")\n",
    "    print(\"bfloat16 autocast ممکن است پشتیبانی شود\")\n",
    "except Exception as e:\n",
    "    print(\"bfloat16 autocast پشتیبانی نمی‌شود:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f538e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "origin_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c36ddfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All parameters: 247577856\tNumber of Trainable: 247577856\n",
      "Percentage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def NumOfTrainableParams(model):\n",
    "    total = origin_model.num_parameters()\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    percent = round(trainable/total*100, 2)\n",
    "    print(f\"Number of All parameters: {total}\\tNumber of Trainable: {trainable}\\nPercentage: {percent}%\")\n",
    "\n",
    "NumOfTrainableParams(origin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e12d1",
   "metadata": {},
   "source": [
    "# Pre-Test the FLAN-T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5fd7019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Write a short summary for this text: #Person1#: Honey, of course I forgive you! I love you so much! I've really missed you. I was wrong to get upset over nothing. \n",
      "#Person2#: I'm sorry I haven't called or anything, but right after you decided you wanted a break, I was called up north to put out some major forest fires! I was in the middle of nowhere, working day and night, trying to prevent the blaze from spreading! It was pretty intense. \n",
      "#Person1#: Oh, honey, I'm glad you're okay! But I have some exciting news. . . I think I'm pregnant! \n",
      "#Person2#: Really? Wow, that's amazing! This is great news! I've always wanted to be a father! We'll go to the doctor first thing in the morning! \n",
      "#Person3#: We have your test results back and, indeed, you are pregnant. Let's see here. . . everything seems to be in order. Your approximate due date is October twenty-seventh two thousand and nine, so that means that the baby was conceived on February third, two thousand and nine. \n",
      "#Person2#: Are you sure? Are these things accurate? \n",
      "#Person3#: Well, yes sir, they are. \n",
      "#Person1#: What's wrong? Why are you asking these questions? \n",
      "#Person2#: This baby isn't mine! I was away the first week of February at a training seminar! \n",
      "#Person1#: I. . . I. . . no, it can't be. . . \n",
      "==================================================\n",
      "Human Summary: #Person1# calls #Person2# to tell him that she was wrong to get upset over nothing and tells him that she's pregnant. They go to see the doctor. However, #Person2# finds out the baby isn't his because he was away the first week of February at a training seminar.\n",
      "\n",
      "Model Summary: The baby was conceived on February 3rd, two thousand and nine.\n"
     ]
    }
   ],
   "source": [
    "_tdial = dataset[\"test\"][421][\"dialogue\"]\n",
    "_tsumm = dataset[\"test\"][421][\"summary\"]\n",
    "_tprompt = (\n",
    "    f\"\"\"\"Write a short summary for this text: {_tdial}\"\"\"\n",
    ")\n",
    "_tinput = tokenizer(_tprompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "_tanswer = tokenizer.decode(\n",
    "    origin_model.generate(_tinput, max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(_tprompt)\n",
    "print(50*\"=\")\n",
    "print(f\"Human Summary: {_tsumm}\\n\\nModel Summary: {_tanswer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37047946",
   "metadata": {},
   "source": [
    "# Fullfiled Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7eb0ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    # return lists (not tensors) so datasets.map handles them properly\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    targets = tokenizer(\n",
    "        example[\"summary\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # replace pad token id in labels with -100 so loss ignores padding\n",
    "    pad_id = tokenizer.pad_token_id or 0\n",
    "    labels = [[(tok if tok != pad_id else -100) for tok in seq] for seq in targets]\n",
    "\n",
    "    return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "\n",
    "ds_tokenized = dataset.map(tokenize_function, batched=True)\n",
    "ds_tokenized = ds_tokenized.remove_columns([\"id\", \"dialogue\", \"summary\", \"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "580fee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of dataset\n",
      " ==================================================\n",
      "Train: (12460, 2)\n",
      "Test: (1500, 2)\n",
      "Validation: (500, 2)\n",
      "tokenized dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n",
      "\n",
      "\n",
      "two samples:\n",
      " [[1363, 5, 3931, 31, 7, 652, 3, 9, 691, 18, 413, 6, 11, 7582, 12833, 77, 7, 7786, 7, 376, 12, 43, 80, 334, 215, 5, 12833, 77, 7, 31, 195, 428, 128, 251, 81, 70, 2287, 11, 11208, 12, 199, 1363, 5, 3931, 10399, 10257, 5, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [8667, 13156, 1217, 11066, 63, 21, 112, 12956, 7, 5, 707, 5, 2737, 7, 11642, 8, 1368, 11, 258, 1527, 11066, 63, 3, 9, 12956, 5, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"shapes of dataset\\n\", 50*'=')\n",
    "print(f'Train: {ds_tokenized['train'].shape}')\n",
    "print(f'Test: {ds_tokenized['test'].shape}')\n",
    "print(f'Validation: {ds_tokenized['validation'].shape}')\n",
    "\n",
    "print(f'tokenized dataset:\\n{ds_tokenized}')\n",
    "print(f'\\n\\ntwo samples:\\n {ds_tokenized['train'][:2]['labels']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86f1ff",
   "metadata": {},
   "source": [
    "## Fine Tune Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c4dc4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure training mode settings\n",
    "origin_model.config.use_cache = False\n",
    "origin_model.config.pad_token_id = tokenizer.pad_token_id  # صریحاً تنظیم کن"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5c7cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All parameters: 251116800\tNumber of Trainable: 3538944\n",
      "Percentage: 1.41%\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# recreate lora model after changing config (اگر قبلاً ساخته شده، دوباره بساز)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=['v', 'q'],\n",
    "    bias='none',\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "lora_model = get_peft_model(origin_model, lora_config)\n",
    "\n",
    "NumOfTrainableParams(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24f8019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "output_dir = f'./saved_models/mtrain-{str(int(time.time()))}'\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=origin_model, pad_to_multiple_of=None)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=2,\n",
    "    # نتابع کردن max_steps یا تنظیم آن مناسب نیاز است؛ max_steps=1 فقط برای تست است\n",
    "    max_steps=10,\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tokenized[\"train\"],\n",
    "    eval_dataset=ds_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# less memory using but it's slower\n",
    "# origin_model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d816a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: total_len=128, non-ignored=48\n",
      "decoded target (non-ignored): Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n",
      "----------------------------------------\n",
      "sample 1: total_len=128, non-ignored=27\n",
      "decoded target (non-ignored): Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.\n",
      "----------------------------------------\n",
      "sample 2: total_len=128, non-ignored=31\n",
      "decoded target (non-ignored): #Person1#'s looking for a set of keys and asks for #Person2#'s help to find them.\n",
      "----------------------------------------\n",
      "median non-ignored tokens in first 200: 34.0\n",
      "tokenizer.pad_token_id: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# چاپ نمونه‌ی برچسب‌ها و تعداد توکن‌های غیر -100\n",
    "for i in range(3):\n",
    "    lbl = ds_tokenized['train'][i]['labels']\n",
    "    non_ignored = [tok for tok in lbl if tok != -100]\n",
    "    print(f\"sample {i}: total_len={len(lbl)}, non-ignored={len(non_ignored)}\")\n",
    "    print(\"decoded target (non-ignored):\", tokenizer.decode(non_ignored, skip_special_tokens=True))\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# آماری روی چند نمونه — اصلاح شده\n",
    "lbls = ds_tokenized['train'][:200]['labels']   # این یک لیست از توکن‌های برچسب هر مثال است\n",
    "counts = [np.sum(np.array(lbl) != -100) for lbl in lbls]\n",
    "print(\"median non-ignored tokens in first 200:\", np.median(counts))\n",
    "\n",
    "# بررسی pad_token_id\n",
    "print(\"tokenizer.pad_token_id:\", tokenizer.pad_token_id)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token or \"<pad>\"\n",
    "    origin_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(\"Set pad_token_id to\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "103da742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual train-step loss: 2.6186940670013428\n"
     ]
    }
   ],
   "source": [
    "# یک قدم آموزشی دستی برای تایید\n",
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(ds_tokenized['train'].select(range(2)), batch_size=1, collate_fn=data_collator)\n",
    "batch = next(iter(dl))\n",
    "lora_model.train()\n",
    "batch_cuda = {k: v.cuda() for k,v in batch.items()}\n",
    "out = lora_model(**batch_cuda)\n",
    "print(\"manual train-step loss:\", out.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a2ddaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 2.9309, 'train_samples_per_second': 13.648, 'train_steps_per_second': 3.412, 'total_flos': 27825159536640.0, 'train_loss': 0.0, 'epoch': 0.0032102728731942215})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02986a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
